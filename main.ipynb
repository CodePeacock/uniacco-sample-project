{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VVXLWTKdFsW",
        "outputId": "92c8fde0-de89-42bb-cb12-797f37fd8cc9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WHq2QoW6c46C"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import random\n",
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    BertForSequenceClassification,\n",
        "    BertTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "\n",
        "# from torch.utils.data import DataLoader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9BRFcJ_c46H",
        "outputId": "5bfa62af-17db-43bd-f64f-5af3673db4f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Load 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(\n",
        "    subset=\"all\",\n",
        "    categories=[\"sci.space\", \"rec.sport.hockey\", \"talk.politics.guns\", \"rec.autos\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XpnX8tOfc46I"
      },
      "outputs": [],
      "source": [
        "# Create a pandas dataframe from the dataset\n",
        "df = pd.DataFrame({\"text\": newsgroups.data, \"label\": newsgroups.target})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpWTfB2Lc46I",
        "outputId": "f32d2ce4-f027-4d99-b568-8769d1454138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-969eba4d6e23>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[\"text\"] = df[\"text\"].str.replace(r\"[^\\w\\s]\", \"\")  # Remove punctuation and digits\n",
            "<ipython-input-22-969eba4d6e23>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df[\"text\"] = df[\"text\"].str.replace(r\"\\d+\", \"\")\n",
            "loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/vocab.txt\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at None\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/tokenizer_config.json\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/config.json\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\",\n",
            "    \"3\": \"LABEL_3\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2,\n",
            "    \"LABEL_3\": 3\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.26.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b/pytorch_model.bin\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the text data\n",
        "df[\"text\"] = df[\"text\"].str.lower()  # Lowercase text\n",
        "df[\"text\"] = df[\"text\"].str.replace(r\"[^\\w\\s]\", \"\")  # Remove punctuation and digits\n",
        "df[\"text\"] = df[\"text\"].str.replace(r\"\\d+\", \"\")\n",
        "df[\"text\"] = df[\"text\"].apply(word_tokenize)  # Tokenize text\n",
        "stop_words = set(stopwords.words(\"english\"))  # Remove stopwords\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x: \" \".join(x))  # Join tokens back into strings\n",
        "df[\"text\"] = df[\"text\"].str.strip()  # Strip whitespace\n",
        "\n",
        "# Split the data into training and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", num_labels=len(train_df[\"label\"].unique())\n",
        ")\n",
        "\n",
        "# Freeze the base BERT layers\n",
        "for param in model.base_model.parameters():\n",
        "    param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HYn0flolc46J"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text data for both the training and test sets\n",
        "train_encodings = tokenizer(\n",
        "    train_df[\"text\"].tolist(),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,  # Set the maximum sequence length to 512\n",
        ")\n",
        "test_encodings = tokenizer(\n",
        "    test_df[\"text\"].tolist(),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=512,  # Set the maximum sequence length to 512\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEUgr0gfc46J",
        "outputId": "22ce5ca6-e35d-45ee-a767-12398029f40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ]
        }
      ],
      "source": [
        "# Define a custom PyTorch dataset for the 20 Newsgroups dataset\n",
        "class NewsGroupDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {\n",
        "            key: torch.tensor(val[idx])\n",
        "            for key, val in self.encodings.items()\n",
        "            if key != \"overflowing_tokens\"\n",
        "        }\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "# Convert the tokenized data into PyTorch datasets\n",
        "train_dataset = NewsGroupDataset(train_encodings, train_df[\"label\"].tolist())\n",
        "test_dataset = NewsGroupDataset(test_encodings, test_df[\"label\"].tolist())\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "# Define the training arguments for the Trainer object\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    max_steps=1000,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=5e-5, eps=1e-8  # Increase learning rate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sLaQYf5Ec46L",
        "outputId": "9b458325-0960-48b7-96bf-4f2c92002079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "Using cuda_amp half precision backend\n",
            "***** Running training *****\n",
            "  Num examples = 3108\n",
            "  Num Epochs = 11\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1000\n",
            "  Number of trainable parameters = 3076\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 08:34, Epoch 10/11]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.404500</td>\n",
              "      <td>1.388690</td>\n",
              "      <td>0.268638</td>\n",
              "      <td>0.295220</td>\n",
              "      <td>0.268638</td>\n",
              "      <td>0.115468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.372400</td>\n",
              "      <td>1.365763</td>\n",
              "      <td>0.341902</td>\n",
              "      <td>0.346891</td>\n",
              "      <td>0.341902</td>\n",
              "      <td>0.260231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.354600</td>\n",
              "      <td>1.352365</td>\n",
              "      <td>0.361183</td>\n",
              "      <td>0.380752</td>\n",
              "      <td>0.361183</td>\n",
              "      <td>0.332156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.342400</td>\n",
              "      <td>1.341477</td>\n",
              "      <td>0.354756</td>\n",
              "      <td>0.437490</td>\n",
              "      <td>0.354756</td>\n",
              "      <td>0.295747</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.344000</td>\n",
              "      <td>1.326827</td>\n",
              "      <td>0.389460</td>\n",
              "      <td>0.506140</td>\n",
              "      <td>0.389460</td>\n",
              "      <td>0.345730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.315500</td>\n",
              "      <td>1.311633</td>\n",
              "      <td>0.438303</td>\n",
              "      <td>0.506034</td>\n",
              "      <td>0.438303</td>\n",
              "      <td>0.414737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.314400</td>\n",
              "      <td>1.300544</td>\n",
              "      <td>0.502571</td>\n",
              "      <td>0.512151</td>\n",
              "      <td>0.502571</td>\n",
              "      <td>0.486865</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.319500</td>\n",
              "      <td>1.293889</td>\n",
              "      <td>0.501285</td>\n",
              "      <td>0.539949</td>\n",
              "      <td>0.501285</td>\n",
              "      <td>0.485408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.289400</td>\n",
              "      <td>1.288447</td>\n",
              "      <td>0.519280</td>\n",
              "      <td>0.535797</td>\n",
              "      <td>0.519280</td>\n",
              "      <td>0.503882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.291700</td>\n",
              "      <td>1.286644</td>\n",
              "      <td>0.519280</td>\n",
              "      <td>0.541876</td>\n",
              "      <td>0.519280</td>\n",
              "      <td>0.503962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.298200</td>\n",
              "      <td>1.286600</td>\n",
              "      <td>0.517995</td>\n",
              "      <td>0.541902</td>\n",
              "      <td>0.517995</td>\n",
              "      <td>0.502988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to ./results/checkpoint-98\n",
            "Configuration saved in ./results/checkpoint-98/config.json\n",
            "Model weights saved in ./results/checkpoint-98/pytorch_model.bin\n",
            "Deleting older checkpoint [results/checkpoint-686] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Saving model checkpoint to ./results/checkpoint-196\n",
            "Configuration saved in ./results/checkpoint-196/config.json\n",
            "Model weights saved in ./results/checkpoint-196/pytorch_model.bin\n",
            "Deleting older checkpoint [results/checkpoint-784] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-294\n",
            "Configuration saved in ./results/checkpoint-294/config.json\n",
            "Model weights saved in ./results/checkpoint-294/pytorch_model.bin\n",
            "Deleting older checkpoint [results/checkpoint-882] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-392\n",
            "Configuration saved in ./results/checkpoint-392/config.json\n",
            "Model weights saved in ./results/checkpoint-392/pytorch_model.bin\n",
            "Deleting older checkpoint [results/checkpoint-980] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-490\n",
            "Configuration saved in ./results/checkpoint-490/config.json\n",
            "Model weights saved in ./results/checkpoint-490/pytorch_model.bin\n",
            "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-588\n",
            "Configuration saved in ./results/checkpoint-588/config.json\n",
            "Model weights saved in ./results/checkpoint-588/pytorch_model.bin\n",
            "Deleting older checkpoint [results/checkpoint-98] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-686\n",
            "Configuration saved in ./results/checkpoint-686/config.json\n",
            "Model weights saved in ./results/checkpoint-686/pytorch_model.bin\n",
            "Deleting older checkpoint [results/checkpoint-196] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-784\n",
            "Configuration saved in ./results/checkpoint-784/config.json\n",
            "Model weights saved in ./results/checkpoint-784/pytorch_model.bin\n",
            "Deleting older checkpoint [results/checkpoint-294] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-882\n",
            "Configuration saved in ./results/checkpoint-882/config.json\n",
            "Model weights saved in ./results/checkpoint-882/pytorch_model.bin\n",
            "Deleting older checkpoint [results/checkpoint-392] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-980\n",
            "Configuration saved in ./results/checkpoint-980/config.json\n",
            "Model weights saved in ./results/checkpoint-980/pytorch_model.bin\n",
            "Deleting older checkpoint [results/checkpoint-490] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n",
            "Saving model checkpoint to ./results/checkpoint-1000\n",
            "Configuration saved in ./results/checkpoint-1000/config.json\n",
            "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [results/checkpoint-588] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./results/checkpoint-980 (score: 0.5039618328947076).\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 778\n",
            "  Batch size = 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:07]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.2866437435150146, 'eval_accuracy': 0.519280205655527, 'eval_precision': 0.5418755643622584, 'eval_recall': 0.519280205655527, 'eval_f1': 0.5039618328947076, 'eval_runtime': 8.6098, 'eval_samples_per_second': 90.362, 'eval_steps_per_second': 1.51, 'epoch': 10.2}\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "model = model.to(device)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    optimizers=(optimizer, None),\n",
        "    compute_metrics=lambda pred: {\n",
        "        \"accuracy\": accuracy_score(pred.label_ids, pred.predictions.argmax(axis=1)),\n",
        "        \"precision\": precision_score(\n",
        "            pred.label_ids, pred.predictions.argmax(axis=1), average=\"weighted\"\n",
        "        ),\n",
        "        \"recall\": recall_score(\n",
        "            pred.label_ids, pred.predictions.argmax(axis=1), average=\"weighted\"\n",
        "        ),\n",
        "        \"f1\": f1_score(\n",
        "            pred.label_ids, pred.predictions.argmax(axis=1), average=\"weighted\"\n",
        "        ),\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "eval_results = trainer.evaluate(test_dataset)\n",
        "print(eval_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl6q5SQjc46N",
        "outputId": "700c9bfa-a48c-4eac-84e6-fd30254faf93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample text: erniecraycom ernest smith subject aftermarket ac units originator ernieferris lines nntppostinghost ferriscraycom organization cray research inc distribution usa article qcaueinnmtaxoncsuncedu andrew brandt writes looked getting ac installed honda crx si unit plus shipping installation like hours top hunk change anyone know place aftermarket ac installation honda ac unit third party unit seem find anyone put third party ac unit honda carolina would prefer place nearby references would handy thx andy brandtcsuncedu les bartels comments sorry cant help question comment make concerning aftermarket ac units frostking frosttemp forget aftermarket unit cavalier quite unhappy fan noisy doesnt put much air never aftermarket ac installed vehicles cant trust quality performance experience les les bartel im going live forever let add ac installed ford garage work well ac installed factory pickups identical mine talked people result dont know problem ford ernie smith\n",
            "True label: rec.autos\n",
            "Predicted label: rec.sport.hockey\n"
          ]
        }
      ],
      "source": [
        "# Test the model on a random sample from the test set\n",
        "sample_index = random.randint(0, len(test_df) - 1)\n",
        "sample_text = test_df.iloc[sample_index][\"text\"]\n",
        "sample_label = test_df.iloc[sample_index][\"label\"]\n",
        "print(\"Sample text:\", sample_text)\n",
        "print(\"True label:\", newsgroups.target_names[sample_label])\n",
        "\n",
        "\n",
        "sample_encoding = tokenizer.encode_plus(\n",
        "    sample_text, truncation=True, padding=True, return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    model_and_encoding = {\"model\": model, \"encoding\": sample_encoding}\n",
        "\n",
        "    torch.save(model_and_encoding, \"model_and_encoding.pt\")\n",
        "\n",
        "\n",
        "# Load the saved model and encoding\n",
        "model_and_encoding = torch.load(\"model_and_encoding.pt\")\n",
        "saved_model = model_and_encoding[\"model\"]\n",
        "saved_encoding = model_and_encoding[\"encoding\"]\n",
        "\n",
        "# Move encoding tensor to the same device as the saved model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "saved_encoding = {k: v.to(device) for k, v in saved_encoding.items()}\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    output = saved_model(**saved_encoding)\n",
        "\n",
        "predicted_label = output[0].argmax().item()\n",
        "print(\"Predicted label:\", newsgroups.target_names[predicted_label])\n"
      ]
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  }
}